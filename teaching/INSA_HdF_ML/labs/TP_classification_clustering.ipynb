{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijujzA9j-1WN"
   },
   "source": [
    "# Machine Learning I - Linear Models\n",
    "\n",
    "Andrés F. LOPEZ-LOPERA <br/>\n",
    "Université Polytechnique Hauts-de-France\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    For this lab session, you are free to use the language of your choice but Python is strongly recommended. In this notebook we will focus on Python implementations based on the toolboxes 'numpy', 'matplotlib', 'pandas' and 'sklearn'.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np              # toolbox with comprehensive mathematical functions\n",
    "import matplotlib.pyplot as plt # toolbox for plotting figures\n",
    "from matplotlib.colors import ListedColormap # to fix some colormaps\n",
    "import pandas as pd             # toolbox for managing dataframes\n",
    "import seaborn as sns           # toolbox for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook focuses on exercises related to classification and clustering models. More precisely, we will explore the following three applications:\n",
    "\n",
    "- Linear models for classification\n",
    "    - Linear discriminant analysis (LDA)\n",
    "    - Logistic regression\n",
    "- Clustering\n",
    "    - $k$-means\n",
    "    - Gaussian mixture models (GMMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi5-UAY9_Qlf"
   },
   "source": [
    "## Exercice 1: Linear models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes dataset\n",
    "\n",
    "In this exercise, we will use the **Iris dataset**, a well-known dataset in the machine learning community. It is readily available in the `sklearn` library. You can find more details in the official documentation here: [Iris dataset Documentation](https://scikit-learn.org/1.5/datasets/toy_dataset.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 (data Analysis).** Load the **Iris dataset** and create a Pandas DataFrame with appropriately named columns for $(X, y)$. Use Python visualization tools (e.g., those covered in the *Python M1* course) to analyze the dataset.  \n",
    "\n",
    "Write a brief report (maximum 10 lines) summarizing your findings, making explicit references to the figures included in your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets     # toolbox ML + datasets\n",
    "\n",
    "# Loading the diabetes dataset\n",
    "dataset_full = \n",
    "dataset_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe\n",
    "dataset = pd.DataFrame()\n",
    "dataset = dataset.set_axis(dataset_full.feature_names + ['y'], axis = 1)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the design matrix\n",
    "pattern_names = # names of the features\n",
    "target_names =  # names of the classes\n",
    "n, d =  # (nb of observations, input dimension + output)\n",
    "K = # nb of classes\n",
    "\n",
    "print(\"Classes:\", target_names)\n",
    "print(\"Patterns:\", pattern_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to add your plots here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(too add your report here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear discriminant analysis in $\\mathbb{R}^d$ for a binary classification\n",
    "\n",
    "The probability density function of the observation $X$ given the class label $y \\in \\{0, 1\\}$ follows a Gaussian distribution with a common covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$:\n",
    "\n",
    "\\begin{align}\n",
    "\tf_{X| y = k}(x) \n",
    "    = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma|}} \\exp\\left(- \\frac{1}{2}(x-\\mu_k)^\\top \\Sigma^{-1} (x-\\mu_k)\\right),\n",
    "\\end{align}\n",
    "\n",
    "where $\\mu_k \\in \\mathbb{R}^d$ represents the mean vector of the Gaussian distribution for class $k$. We assume prior probabilities $\\mathbb{P}(y = k) = \\pi_k$ for all $k \\in \\{0, 1\\}$.  \n",
    "\n",
    "By applying Bayes' theorem, the log posterior ratio is given by  \n",
    "\n",
    "\\begin{align*}\n",
    "\t\\log \\left(\\frac{\\mathbb{P}(y = 1|x)}{\\mathbb{P}(y = 0|x)}\\right)\n",
    "\t= \\delta_1(x) - \\delta_0(x)\n",
    "\t= \\langle x, \\beta \\rangle + \\beta_0,\n",
    "\\end{align*}\n",
    "\n",
    "where $\\delta_k$ is the linear discriminant function defined as  \n",
    "\n",
    "\\begin{align*}\n",
    "    \\delta_k(x) \n",
    "\t&= \\langle x, \\Sigma^{-1} \\mu_k \\rangle - \\frac{1}{2} \\mu_k^\\top \\Sigma^{-1} \\mu_k + \\log \\left(\\pi_k \\right).\n",
    "\\end{align*}\n",
    "\n",
    "Within this context, the classification rule is given by\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\eta_{\\theta}(x) = \\mathbb{1}_{\\mathbb{P}(y = 1|x) \\geq \\mathbb{P}(y = 0|x)}\n",
    "\t\\quad\n",
    "\t\\Leftrightarrow\n",
    "\t\\quad\n",
    "\t\\eta_{\\theta}(x) = \\mathbb{1}_{\\delta_1(x) \\geq \\delta_0(x)}.\n",
    "\\end{equation*}\n",
    "\n",
    "#### Parameter estimators\n",
    "\n",
    "The parameters of the LDA model can be estimated using the maximum likelihood method. The first-order optimality conditions yield the following estimators:  \n",
    "\n",
    "1. **Class Prior Probabilities:** The empirical estimates of the prior probabilities are given by  \n",
    "\n",
    "   $$\n",
    "   \\hat{\\pi}_k = \\frac{n_k}{n}, \\quad k \\in \\{0,1\\},\n",
    "   $$\n",
    "   \n",
    "   where $n_k$ is the number of observations in class $k$, and $n$ is the total number of observations.\n",
    "\n",
    "2. **Class Means:** The maximum likelihood estimates of the class means are  \n",
    "   \n",
    "   $$\n",
    "   \\hat{\\mu}_k = \\sum_{i: y_i = k} \\frac{x_i}{n_k}, \\quad k \\in \\{0,1\\}.\n",
    "   $$\n",
    "\n",
    "3. **Covariance Matrix:** Since LDA assumes a common covariance matrix across both classes, the pooled sample covariance estimate is  \n",
    "   \n",
    "   $$\n",
    "    \\hat{\\Sigma}_k = \\sum_{i : y_i = k} \\frac{(x_i-\\hat\\mu_k)(x_i-\\hat\\mu_k)^\\top}{n_k-1}, \\quad\n",
    "\t\\hat{\\Sigma} = \\sum_{k = 0}^{1}  \\frac{n_k-1}{n-2} \\hat{\\Sigma}_k, \\quad k \\in \\{0,1\\}.\n",
    "   $$\n",
    "\n",
    "These estimates can then be used to construct the linear discriminant functions and derive the corresponding decision rule.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (Implemention of LDA in $\\mathbb{R}^2$ from Scratch).** Consider a binary classification problem involving only the `setosa` and `versicolor` classes (i.e., exclude data related to `virginica`). Focus on the features `petal length (cm)` and `petal width (cm)`.  \n",
    "\n",
    "Using the formulas derived in the course, compute the estimators of the following parameters:  \n",
    "- Class priors: $\\hat{\\pi}_k$  \n",
    "- Class means: $\\hat{\\mu}_k$  \n",
    "- Class covariance matrices: $\\hat{\\Sigma}_k$  \n",
    "- Shared covariance matrix: $\\hat{\\Sigma}$  \n",
    "\n",
    "In a single panel, create a scatterplot of the data points and display the estimated means $\\hat{\\mu}_k$. Additionally, visualize the estimated local covariances $\\hat{\\Sigma}_k$ by plotting ellipsoids using four contour levels. The `Ellipse` function from `matplotlib.patches` can be useful for this purpose.  \n",
    "\n",
    "Finally, apply the LDA classification rule to compute predictions and report the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points(X, y, target_names):\n",
    "    for k in range(len(target_names)):\n",
    "        ax.scatter(X[y == k, 0], X[y == k, 1], label = target_names[k], s = 20)\n",
    "    ax.set(xlabel = dataset_full.feature_names[2],\n",
    "           ylabel = dataset_full.feature_names[3])\n",
    "    plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# Function to plot Gaussian ellipses\n",
    "def plot_LDA_ellipses(means, covariances, ax):\n",
    "    colors = sns.color_palette(\"colorblind\", 2)\n",
    "    for i, (mean, covar) in enumerate(zip(means, covariances)):\n",
    "        eigenvalues, eigenvectors = \n",
    "        order = eigenvalues.argsort()[::-1]\n",
    "        eigenvalues, eigenvectors = eigenvalues[order], eigenvectors[:, order]\n",
    "        angle = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))\n",
    "        volume = \n",
    "        width, height = \n",
    "        for ncontour in range(4):\n",
    "            ellipse = Ellipse(mean, (ncontour+1)*width, (ncontour+1)*height, angle = angle, \n",
    "                              edgecolor = colors[i], facecolor = colors[i], lw = 2, alpha = 0.2)\n",
    "            ax.add_patch(ellipse);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_names_2d = # defining target features\n",
    "target_names_bin = # defining target classes\n",
    "\n",
    "X = # extracting X related to the petal \n",
    "y = # extracting y  \n",
    "\n",
    "idx_keep = # discarding data with class \"setosa\" \n",
    "y = y[idx_keep]    # to ensure y \\in {0, 1} \n",
    "X = X[idx_keep, :]\n",
    "nbin = X.shape[0]  # (nb of observations, input dimension + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_hat = [None] * 2                # pi parameters\n",
    "mu_hat = [None] * 2                # mean parameters\n",
    "Sigma_hat_local = [None] * 2       # local covariance parameters\n",
    "Sigma_hat_shared = np.zeros((2,2)) # shared covariance parameter\n",
    "\n",
    "for k in range(2):\n",
    "    pi_hat[k] = \n",
    "    mu_hat[k] = \n",
    "    Sigma_hat_local[k] = \n",
    "    Sigma_hat_shared += "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_points(X, y, target_names_bin)\n",
    "plot_LDA_ellipses(mu_hat, Sigma_hat_local, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = \n",
    "print(\"accuracy:\", np.sum(y_pred == y) / nbin)\n",
    "print(\"accuracy (sklearn):\", accuracy_score(y_pred, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(too add your comments here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (plotting the decision boundary).** The function `frontier` visualizes the decision boundary of a classifier by evaluating it on a grid of points, considering the range of input variables in the dataset $X$. It requires the following arguments:  \n",
    "- `X` : feature matrix $X \\in \\mathbb{R}^{n \\times d}$  \n",
    "- `y` : fround truth labels $y \\in \\mathbb{N}$  \n",
    "- `resolution` : number of points per dimension to construct the grid  \n",
    "- `f` : classification function $f: \\mathbb{R}^{d} \\to \\mathbb{N}$, which assigns a class label to a given input  \n",
    "\n",
    "Modify your previous code to ensure compatibility with `frontier`, then use it to visualize the decision boundary. Finally, display the results and analyze your observations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frontier(f, X, y, step = 100):\n",
    "    \"\"\" function for plotting the decision frontier of a binary classifier f \"\"\"    \n",
    "    # converting data to np.arrays\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # defining a grid of test points for evaluation\n",
    "    eps = 0.1\n",
    "    min_x0, max_x0 = np.min(X[:, 0])-eps, np.max(X[:, 0])+eps\n",
    "    min_x1, max_x1 = np.min(X[:, 1])-eps, np.max(X[:, 1])+eps\n",
    "    x1, x2 = np.meshgrid(np.arange(min_x0, max_x0, (max_x0 - min_x0) / step),\n",
    "                         np.arange(min_x1, max_x1, (max_x1 - min_x1) / step))\n",
    "    \n",
    "    # computing the predictions for each point in the test grid\n",
    "    z = np.array([f(vec) for vec in np.c_[x1.ravel(), x2.ravel()]]).reshape(x1.shape)\n",
    "    pred_labels = np.unique(z)\n",
    "    \n",
    "    #  defining colormap to have similar colors to the points\n",
    "    color_blind_list = sns.color_palette(\"colorblind\", pred_labels.shape[0])\n",
    "    sns.set_palette(color_blind_list)\n",
    "    cmap = ListedColormap(color_blind_list)\n",
    "    \n",
    "    # plotting prediction map \n",
    "    plt.imshow(z, origin = 'lower', extent = [min_x0, max_x0, min_x1, max_x1],\n",
    "               interpolation = 'mitchell', alpha = 0.80, cmap = cmap, aspect='auto')\n",
    "    ax = plt.gca()\n",
    "    cbar = plt.colorbar(ticks = pred_labels)\n",
    "    cbar.ax.set_yticklabels(pred_labels)\n",
    "\n",
    "    labels = np.unique(y).shape[0]\n",
    "    color_blind_list = sns.color_palette(\"colorblind\", labels)\n",
    "    for i, label in enumerate(y):\n",
    "        plt.scatter(X[i, 0], X[i, 1],\n",
    "                    color = color_blind_list[int(y[i])], s = 80)\n",
    "    plt.xlim([min_x0, max_x0])\n",
    "    plt.ylim([min_x1, max_x1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\" My LDA classifier \"\"\"\n",
    "    return()\n",
    "\n",
    "nb_steps = 200 # resolution for the evaluation grid\n",
    "\n",
    "fig = plt.figure()\n",
    "title = \"Accuracy (LDA) \" + \\\n",
    "        \": {:.2f}\".format(accuracy_score(y, y_pred))\n",
    "frontier(f, X, y, nb_steps)\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(too add your comments here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 (LDA using sklearn).** Repeat **Question 3**, this time using the `LinearDiscriminantAnalysis` class from `sklearn` ([LDA Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)). Compare the decision boundaries obtained with those from the previous method. Analyze any differences you observe. \n",
    "\n",
    "  \n",
    "\n",
    "Adapt the code for a classification problem involving three classes: `setosa`, `versicolor`, and `virginica` from the Iris dataset. Report your findings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf_LDA = \n",
    "y_LDA =\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Classifier\"\"\"\n",
    "    return()\n",
    "\n",
    "fig = plt.figure()\n",
    "title = \"Accuracy (LDA) \" + \\\n",
    "        \": {:.2f}\".format(accuracy_score(y, y_LDA))\n",
    "frontier(f, X, y, nb_steps)\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-class model\n",
    "X = # extracting X related to the petal \n",
    "y = # extracting y  \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(too add your comments here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 (Logistic regression).** The logistic classifier is also available in `sklearn` ([LogisticRegression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)). Repeat **Question 4**, this time using the `LogisticRegression` classiffier. Compare the results with those obtained using LDA. Analyze any differences in decision boundaries and classification performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_logistic = LogisticRegression()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(too add your comments here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 ($k$ means from scratch).** Consider the **Iris dataset**. As in **Exercise 1**, focus on the features related to `petal length (cm)` and `petal width (cm)`. Implement Lloyd's algorithm for $k$-means clustering as discussed in the course for $K$ classes. In your experiments, consider $K = 3$. Initialize randomly the centroids of the algorithm. During the process, plot the results of each iteration to visualize the clustering progress. Define a stopping criterion for the algorithm.\n",
    "\n",
    "Repeat the previous procedure for different random initializations and different values of $K$. What can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_euclidean(x, y):\n",
    "    return()\n",
    "\n",
    "def my_kmeans_plot(X, centroids, n_iter_max):\n",
    "    colors = sns.color_palette(\"colorblind\", len(centroids))\n",
    "    \n",
    "    for n in range(n_iter_max):\n",
    "        # Step 1: assigning points to classes according to the neareast centroid\n",
    "        pred = \n",
    "            \n",
    "        # Step 2: updating the centroids\n",
    "        centroids =\n",
    "        \n",
    "        _, ax = plt.subplots()\n",
    "        for k in range(len(target_names)):\n",
    "            ax.scatter(X[pred == k, 0], X[pred == k, 1], label = target_names[k], s = 20)\n",
    "            ax.scatter(centroids[k, 0], centroids[k, 1], marker = 'x', \n",
    "                       s = 80, linewidths = 3, color = colors[k], zorder = 10)\n",
    "        ax.set(xlabel = dataset_full.feature_names[2],\n",
    "               ylabel = dataset_full.feature_names[3])\n",
    "        \n",
    "    return((pred, centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = # extracting X related to the petal \n",
    "n_clusters = # defining nb of clusters\n",
    "\n",
    "# random initialization of the centroids\n",
    "centroids = \n",
    "\n",
    "y_kmeans, centroids_pred = my_kmeans_plot(X, centroids, 5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(too add your comments here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (plotting the decision boundary).** Using the function `frontier`, visualize the decision boundary of the $k$-means clustering algorithm. Given the true labels available in the dataset, compute the accuracy of the model. You may need to adjust the predicted labels to account for any identification issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\" k-means \"\"\"\n",
    "    return() \n",
    "\n",
    "fig = plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (GMM using sklearn).** Repeat **Questions 1-2** using a Gaussian Mixture Model (GMM) with three mixture components. You can use the `GaussianMixture` function available in `sklearn` ([GMM Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)).\n",
    "\n",
    "For displaying the results, first plot the corresponding centers and ellipsoids of each Gaussian component. You may need to adapt the `plot_LDA_ellipses` function from **Exercise 1 (Question 2)** to pass the means and covariances of the GMM. \n",
    "\n",
    "Next, visualize the decision boundary using the `frontier` function from **Exercise 1 (Question 3)**. What can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "colors = sns.color_palette(\"colorblind\", n_clusters)\n",
    "\n",
    "# Function to plot Gaussian ellipses\n",
    "def plot_gmm_ellipses():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# fitting a Gaussian Mixture Model\n",
    "gmm = \n",
    "y_gmm =\n",
    "\n",
    "# plotting points and ellipses\n",
    "fig, ax = plt.subplots()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"weights:\", gmm.weights_)\n",
    "print(\"means:\", gmm.means_)\n",
    "print(\"covariances:\", gmm.covariances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\" k-means \"\"\"\n",
    "    return() \n",
    "\n",
    "# plotting the decision boundary\n",
    "fig = plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(too add your comments here)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yu2_pJAyfh9W"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
