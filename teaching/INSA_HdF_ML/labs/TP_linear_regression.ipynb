{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijujzA9j-1WN"
   },
   "source": [
    "# Machine Learning I - Linear Models\n",
    "\n",
    "Andrés F. LOPEZ-LOPERA <br/>\n",
    "Université Polytechnique Hauts-de-France\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    For this lab session, you are free to use the language of your choice but Python is strongly recommended. In this notebook we will focus on Python implementations based on the toolboxes 'numpy', 'matplotlib', 'pandas' and 'sklearn'.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np              # toolbox with comprehensive mathematical functions\n",
    "import matplotlib.pyplot as plt # toolbox for plotting figures\n",
    "import pandas as pd             # toolbox for managing dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook focuses on exercises related to linear regression models. More precisely, we will explore the following three applications:\n",
    "\n",
    "- Linear regression using least squares (LS)\n",
    "- Linear regression with Ridge and Lasso regularization\n",
    "- Image compression using Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi5-UAY9_Qlf"
   },
   "source": [
    "## Exercice 1: Linear regression using least squares (LS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes dataset\n",
    "\n",
    "In this exercise, we will work with the diabetes dataset, which is frequently used in the machine learning community. This dataset is conveniently available in the `sklearn` toolbox. You can find more details in the documentation here: <a href=\"https://scikit-learn.org/1.5/datasets/toy_dataset.html\" > https://scikit-learn.org/1.5/datasets/toy_dataset.html </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 (data analysis).** Load the diabetes dataset and create a Pandas DataFrame with appropriately named columns for $(X, y)$.\n",
    "\n",
    "Before applying any machine learning techniques, it is important to analyze first the data. This helps build familiarity with the dataset and provides better context for interpreting the results. Feel free to use any Python visualization tools in your analysis (see, e.g., the ones studied in the \"Python M1\" course).\n",
    "\n",
    "Write a short report (max 10 lines) sumarizing your analysis and findings. Don't forget to make reference to the figures proposed in your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets     # toolbox ML + datasets\n",
    "\n",
    "# Loading the diabetes dataset\n",
    "dataset_full = \n",
    "names_patterns = # extracting names of the features\n",
    "dataset_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an easier visualization and manupulation of the dataset, we can opt creating a (pandas) dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe\n",
    "dataset = pd.DataFrame()\n",
    "dataset = dataset.set_axis()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(too add your report here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares in $\\mathbb{R}^d$\n",
    "\n",
    "We consider the linear model\n",
    "\n",
    "\\begin{equation}\n",
    "    y(x) = \\beta_0 + \\sum_{j = 1}^{d} \\beta_j x_j + \\varepsilon,\n",
    "\\end{equation}\n",
    "\n",
    "with $x = (x_1, \\ldots, x_d) \\in \\mathbb{R}^{d}$ (co-variables), $\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_d) \\in \\mathbb{R}^{p}$ (coefficient parameters) and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ (additive Gaussian noise).\n",
    "\n",
    "If $X^\\top X$ is a full rank matrix, then one can show that the LS estimator of $\\beta$ is given by\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\widehat{\\beta} = (X^\\top X)^{-1} X^\\top y,\n",
    "\\end{equation*}\n",
    "\n",
    "Once $\\widehat{\\beta}$ is computed, then the prediction $\\widehat{y}$ of $y$ at $x \\in \\mathbb{R}^d$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    y(x) = \\widehat{\\beta}_0 + \\sum_{j = 1}^{d} \\widehat{\\beta}_j x_j.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (LS in  $\\mathbb{R}$ from scratch).** Consider a simple linear regression model for each variable: `age`, `bmi` and `bp`. Using the formulas derived in the course, compute the LS estimator for $\\beta$.\n",
    "\n",
    "On the same plot, display the observed data $y$ along with the predicted values $\\widehat{y}$. Then, compute and print the following performance metrics: $\\operatorname{MSE}$ (Mean Squared Error), $\\operatorname{SMSE}$ (Standardized Mean Squared Error), and $R^2$ (coefficient of determination). For the latter metrics, propose proper python functions with a short description explaining the input/output arguments. \n",
    "\n",
    "Analyze and discuss the results. What observations can you make from the metrics and plots? What conclusions can you draw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the design matrix\n",
    "X = dataset[names_patterns]\n",
    "y = np.array(dataset['y'])\n",
    "n, d = X.shape # (nb of observations, input dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining functions for computing the performance indicators\n",
    "def MSE(y, y_pred):\n",
    "    \"\"\" Compute the Mean Squared Error (MSE) \"\"\"\n",
    "    return()\n",
    "\n",
    "def SMSE(y, y_pred):\n",
    "    \"\"\" Compute the Standardized Mean Squared Error (SMSE) \"\"\"    \n",
    "    return()\n",
    "\n",
    "def R2(y, y_pred):\n",
    "    \"\"\" Compute the coefficient of determination $R^2$ \"\"\"\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "names_patterns_short = ['age', 'bmi', 'bp']\n",
    "\n",
    "for pattern in names_patterns_short:\n",
    "    # extracting data for the target pattern\n",
    "    x = \n",
    "    \n",
    "    # computing the LS estimator\n",
    "    beta = \n",
    "    \n",
    "    # computing the predictions\n",
    "    y_pred = \n",
    "    \n",
    "    # plotting the prediction\n",
    "    fig = plt.figure()\n",
    "    plt.show()\n",
    "    \n",
    "    print(pattern)\n",
    "    print(\"beta =\", beta)\n",
    "    print(\"[MSE, SMSE, R2] =\", [MSE(y, y_pred), SMSE(y, y_pred), R2(y, y_pred)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (LS in  $\\mathbb{R}^d$ from scratch).** Now, consider a linear regression model that accounts for the three variables: `age`, `bmi` and `bp`. Compute the LS estimator for $\\beta$.\n",
    "\n",
    "Create a scatter plot comparing the predicted values $\\widehat{y}$ to the ground truth observations $y$. This plot is known as a *calibration plot*. What patterns or insights can you observe?\n",
    "\n",
    "Finally, compute and print the performance metrics: $\\operatorname{MSE}$ (Mean Squared Error), $\\operatorname{SMSE}$ (Standardized Mean Squared Error), and $R^2$ (coefficient of determination). Discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting data for the patterns [\"age\", \"bmi\", \"bp\"]\n",
    "pattern = [\"age\", \"bmi\", \"bp\"]\n",
    "x = \n",
    "beta = \n",
    "y_pred = \n",
    "  \n",
    "# plotting predictions vs observations\n",
    "fig = plt.figure()\n",
    "plt.show()\n",
    "    \n",
    "print(\"beta =\", beta)\n",
    "print(\"[MSE, SMSE, R2] =\", [MSE(y, y_pred), SMSE(y, y_pred), R2(y, y_pred)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 (LS using sklearn).** Repeat **Questions 1-3** using the utilities provided in `sklearn`. Refer to the following documentation for guidance:\n",
    "\n",
    "- <a href=\"https://scikit-learn.org/1.5/modules/linear_model.html\" > https://scikit-learn.org/1.5/modules/linear_model.html </a>\n",
    "- <a href=\"https://scikit-learn.org/1.6/modules/model_evaluation.html\" > https://scikit-learn.org/1.6/modules/model_evaluation.html </a>\n",
    "\n",
    "Compare the results from your implementations with those obtained using `sklearn`. Do you notice any differences? If so, what might explain them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import \n",
    "from sklearn.metrics import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 (model selection).** Propose a new linear model but this time considering all the input variables available in the dataset. What observations and conclusions can you draw? Are all the input variables necessary for the model? Justify your answer by applying a forward (or backward) selection routine for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: Linear regression via Ridge and Lasso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating toy data\n",
    "\n",
    "As discussed in the course, we can generate toy data by sampling $X \\in \\mathbb{R}^{n \\times p}$ from a Gaussian distribution. We can assume for instance that $x_{i,j}$ are independent and identically distributed (i.i.d.) random variables following the distribution $x_{i,j} \\sim \\mathcal{N}(0, 1)$, where $i \\in {1, \\ldots, n}$ and $j \\in {1, \\ldots, p}$.\n",
    "\n",
    "Given a fixed $\\beta_\\star \\in \\mathbb{R}^p$, we then have that:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = X \\beta_\\star.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 (data generation).** Consider $\\beta_\\star \\in \\mathbb{R}^p$ as a vector where all elements are zero, except for the first eight (8) terms, which are defined by your birthday sequence: $\\beta = (d_1, d_2, m_1, m_2, y_1, y_2, y_3, y_4, 0, \\ldots, 0)$, where $(d_1 d_2)$, $(m_1 m_2)$, and $(y_1 y_2 y_3 y_4)$ represent the day, month, and year of your birth, respectively.\n",
    "\n",
    "Generate a toy dataset by sampling a (pseudo-)random matrix $X \\in \\mathbb{R}^{51 \\times 50}$ with $x_{i,j} \\sim \\mathcal{N}(0, 1)$. To ensure reproducibility, set a random seed for the generation of $X$. Compute the target values $y$ using the linear model, and then add additive noise to the observations:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = X \\beta_\\star + \\varepsilon,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\varepsilon \\sim \\mathcal{N}(0, \\tau^2 I)$ with $\\tau^2 = 1$.\n",
    "\n",
    "The resulting dataset $(X, y)$ will be used for the subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the \"size\" of the problem\n",
    "p =  # nb features \n",
    "n =  # nb obs\n",
    "\n",
    "# generating pseudo-random data for a given beta\n",
    "beta_true = \n",
    "\n",
    "X = \n",
    "y_true = \n",
    "\n",
    "# adding noise\n",
    "var_sigma = \n",
    "noise = \n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (Ridge using SVD).** Using the SVD framework, compute the Ridge estimator for different values of the regularization parameter $\\lambda \\in [10^{-2}, 10^{4}]$. Consider a grid of $100$ values of $\\lambda$'s in a log-scale. By following the example shown in the course, display the evolution of each coefficient $\\beta_i$ for all $i \\in {1, \\ldots, p}$ as a function of $\\lambda$ in a single plot.\n",
    "\n",
    "For this procedure, implement a function `ridge_path` that takes as input $X$, $y$ and a vector containing the regularization parameters $\\lambda$. The function should return a matrix containing the estimated coefficients $\\beta$ for each value of $\\lambda$. Be sure to include a brief description of the function, this is always useful when allowing people to use your implementations!\n",
    "\n",
    "Compute the predictions $\\widehat{y}$ obtained when considering $\\widehat{\\lambda}_{CV}$. Display a calibration plot together to some error indicators. What patterns or insights can you observe? What can be said about the cardinality of $\\widehat{\\beta}_{\\widehat\\lambda_{CV}}$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_path(X, y, alphas):\n",
    "    \"\"\" compute the ridge path for a list of tuning parameters \"\"\"\n",
    "    \n",
    "    return beta_ridge\n",
    "\n",
    "# defining the grid for the Ridge parameter (\\lambda in the course)\n",
    "alpha_max = \n",
    "alpha_min = \n",
    "n_alphas = \n",
    "\n",
    "alphas = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_ridge = ridge_path(X, y, alphas)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (cross-validation).** Estimate $\\lambda$ via hold-out validation with $K = 2, 5, 10$. You can use the function `RidgeCV` from `sklearn.linear_model`. Display again the evolution of $\\beta$ as a function of $\\lambda$, while plotting the value of the estimated $\\widehat{\\lambda}_{CV}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting predictions vs observations\n",
    "y_pred = \n",
    "fig = plt.figure()\n",
    "plt.show()\n",
    "\n",
    "print(\"[MSE, SMSE, R2] =\", )\n",
    "print(\"|beta| =\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 (Lasso).** Repeat **Questions 2-3**, this time using Lasso regression. You can use the functions available in `sklearn`. For more details, refer to the documentation here: <a href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Lasso.html\" > https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Lasso.html </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 (LSLasso).** Implement LSLasso and compare the results with those obtained using Ridge and Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Image compression via SVD\n",
    "\n",
    "In this exercise, we will explore how Singular Value Decomposition (SVD) can be used to \"compress\" a graphical figure. By representing the figure as a matrix and applying the SVD, we can identify the closest matrix of lower rank to the original. This method serves as a foundation for efficient compression techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, manipulate and display the image\n",
    "\n",
    "This NASA photo comes from the Hubble telescope and presents a dramatic picture of this extra-galactic formation.\n",
    "\n",
    "![TarantulaNebula.jpg](images/TarantulaNebula.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa = mpimg.imread(\"images/TarantulaNebula.jpg\")\n",
    "print(nasa.shape)\n",
    "plt.imshow(nasa) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Extract and comment the information relative to the nasa matrix: dimensions, min and max values of the elements,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also transform the RBG image into a greyscale image. To simplify this exercise, we turn the nasa image into a greyscale with ordinary double precision values 0-255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_greyscale = nasa[:,:,0] + nasa[:,:,1] + nasa[:,:,2]   # to sump up red + green + blue channels\n",
    "img_greyscale = img_greyscale*255 / np.max(img_greyscale) # to make this bebright white\n",
    "\n",
    "plt.imshow(img_greyscale, cmap='gray') \n",
    "plt.show()\n",
    "\n",
    "# Remark:\n",
    "# imshow() can take as entry an image of RGB values (shape=(dim1, dim2, 3)) or an image of \n",
    "# scalar value (shape=(dim1, dim2)). If we pass an image of RGB values, the cmap parameter will be ignore.\n",
    "# If we pass an image of scalar value and let the default value of the cmap parameter, imshow() will\n",
    "# map scalar data to colors, so to have a greyscale_img we need to set cmap to 'gray'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD decomposition\n",
    "\n",
    "For any matrix $X \\in \\mathbb{R}^{n \\times p}$, there exists two orthonormal matrices $U = [u_1, \\ldots, u_n] \\in \\mathbb{R}^{n \\times n}$ and $V = [v_1, \\ldots, v_p] \\in \\mathbb{R}^{p \\times p}$ such that\n",
    "\n",
    "\\begin{equation*}\n",
    "\tX = U \\Sigma V^\\top, %\\in \\bbR^{n \\times p},\n",
    "\\end{equation*}\n",
    "\n",
    "with $\\Sigma = \\operatorname{diag}(s_1, \\ldots, s_{r})$, $s_1 \\geq \\cdots \\geq s_{r} > 0$, and $r = \\operatorname{rang}(X)$.\n",
    "\n",
    "Here, \n",
    "- The unitary matrix $V$ contains the « right-singular vectors » of $X$.\n",
    "- The unitary matrix $U$ contains the « left-singular vectors » of $X$.\n",
    "- The diagonal matrix $\\Sigma$ contains the singular values of the $X$. They correspond to the roots of the eigenvalues of $X^\\top X$. The number of non-zero singular values is equal to the rank of $X$. As a convention we order $\\Sigma_{i,i}$ by decreasing order.\n",
    "\n",
    "We call this factorization the sigular value decomposition (SVD) of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Perform the SVD decomposition of the greyscale image by using the `svd` command from the numpy linear algrebra library. Check the dimensions of the decomposition outputs. </p>\n",
    "\n",
    "See the documentation here: <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html\" > https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the SVD to the img_greyscale \n",
    "U, s, Vt = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(U)\n",
    "print(np.shape(U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(s))\n",
    "plt.semilogy(s,'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Vt)\n",
    "print(np.shape(Vt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction of the image using some of the singular values\n",
    "\n",
    "We can verify that the image $X$ can be reconstructed from the SVD decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we will construct the S matrix from the s vector (which corresponds to the diagonal terms)\n",
    "S =\n",
    "print('matrix S', S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_greyscale_recomposed = \n",
    "\n",
    "plt.imshow(img_greyscale_recomposed, cmap='gray')\n",
    "plt.title('Reconstructed full SVD')\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE:\", np.mean((img_greyscale - img_greyscale_recomposed)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** Compare three recomposed matrices:\n",
    "\n",
    "- using the first $r$ elements (reduced SVD)\n",
    "- using the first 300 elements\n",
    "- using the first 50 elements\n",
    "\n",
    "Plot the associated images and comment the obtained images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** Design an automatic rule for the selection of the number of singular values used in the reconstruction of the image. Justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 (bonus).** Implement the SVD from scratch by considering the spectral theorem studied in the course for the diagonalization of a symmetric matrix. Compare the results with those obtained using the command `np.linalg.svd`. Discuss the differences if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yu2_pJAyfh9W"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
